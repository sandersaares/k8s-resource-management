apiVersion: apps/v1
kind: Deployment
metadata:
  name: victim
spec:
  replicas: 3
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: victim
  template:
    metadata:
      labels:
        app: victim
    spec:
      nodeSelector:
        "kubernetes.io/os": linux
      terminationGracePeriodSeconds: 30
      containers:
      - name: consume
        image: sandersaares/consumer:latest
        resources:
          requests:
            cpu: 1.0
            memory: 20Gi
          limits:
            cpu: 1.0
            memory: 20Gi
        args:
        # We allocate managed memory, whereas the k8s resource management deals with total memory.
        # Therefore, the numbers are offset by a fair margin to allow for managed/native "overhead".
        # The reason for this rather large discrepancy is unclear - on dev PC it seems much less.
        - "--memory-gb=4"
        - "--extra-memory-gb=6"
        - "--extra-memory-delay-seconds=60"
        ports:
        - containerPort: 5000